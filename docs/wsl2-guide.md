# WSL2 å®Œæ•´å®‰è£æŒ‡å—

**ä¸­æ–‡ç‰ˆ | [English](wsl2-guide-en.md)**

> é€™æ˜¯å¾é›¶é–‹å§‹åœ¨ WSL2 å®‰è£ OpenClaw + Ollama çš„å®Œæ•´æŒ‡å—ã€‚

> è‹¥æ‚¨çš„ä½œæ¥­ç³»çµ±æ˜¯ Ubuntu / macOS ï¼Œå¯ä»¥ç›´æ¥å¾ç¬¬2æ­¥é©Ÿé–‹å§‹ã€‚

> å¦‚æœä½ å·²ç¶“åœ¨ Windows åŸç”Ÿç’°å¢ƒå®‰è£éï¼Œè«‹å„ªå…ˆçœ‹ï¼š**`docs/migration-guide.md`**ã€‚

---

## ç›®éŒ„

1. [å®‰è£ WSL2](#1-å®‰è£-wsl2)  
2. [å®‰è£ Ollama](#2-å®‰è£-ollama)  
3. [å®‰è£ OpenClaw](#3-å®‰è£-openclaw)  
4. [é…ç½® Gateway](#4-é…ç½®-gateway)  
5. [æ¸¬è©¦èˆ‡é©—è­‰](#5-æ¸¬è©¦èˆ‡é©—è­‰)  
6. [é€²éšé…ç½®](#6-é€²éšé…ç½®)  
7. [å¸¸è¦‹å•é¡Œ](#7-å¸¸è¦‹å•é¡Œ)

---

Windows (WSL ç‰ˆ) å¿«é€Ÿå®‰è£ OpenClaw èˆ‡æœ¬åœ°ç«¯ LLM (Ollama) çš„å®Œæ•´æ­¥é©ŸæŒ‡å—ã€‚

> âš ï¸ **ç‰ˆæœ¬éœ€æ±‚**: Ollama v0.15.4+ èˆ‡ OpenClaw 2026.2.5+

## 1. å®‰è£ WSL2 (è‹¥æ‚¨ä½¿ç”¨ Windows 10/11ï¼Œè«‹å¾é€™è£¡é–‹å§‹)

### 1.1 å•Ÿç”¨ WSL2

ä»¥ **ä¸€èˆ¬ä½¿ç”¨è€…èº«ä»½** é–‹å•Ÿ Command Prompt:

```cmd
# å®‰è£ WSL2 ä¸¦é–‹å•Ÿ è™›æ“¬ç’°å¢ƒ
wsl --install
wsl --update
```

ä¾æç¤º **é‡æ–°å•Ÿå‹•é›»è…¦**ã€‚

### 1.2 åˆæ¬¡å®‰è£ä¸¦å•Ÿå‹• Ubuntu

é‡é–‹æ©Ÿå¾Œï¼Œä»¥ **ä¸€èˆ¬ä½¿ç”¨è€…èº«ä»½** é–‹å•Ÿ Command Prompt:

```cmd
# å®‰è£ Ubuntu 24.04
wsl --install -d Ubuntu-24.04
```

ä¾æç¤ºè¨­å®šï¼š

- ä½¿ç”¨è€…åç¨±ï¼ˆå»ºè­°èˆ‡ Windows ç›¸åŒï¼Œç´”å€‹äººå–œå¥½ï¼‰
- å¯†ç¢¼ï¼ˆä¹‹å¾Œ sudo è¦ç”¨ï¼‰


### 1.3 å•Ÿç”¨ systemdï¼ˆå¿…è¦ï¼‰

> âš ï¸ å¿…åšæ­¥é©Ÿï¼šOpenClaw Gateway éœ€è¦ systemd æ‰èƒ½ç•¶æœå‹™å¸¸é§ã€‚

åœ¨ Ubuntu WSL çµ‚ç«¯æ©ŸåŸ·è¡Œï¼š

```bash
cd ~
sudo tee /etc/wsl.conf >/dev/null <<'EOF'
[boot]
systemd=true
EOF
```

(è‹¥æç¤º `[sudo] password for user:` éœ€è¦å¯†ç¢¼æ™‚ï¼Œè¼¸å…¥ä¸Šé¢çš„å¯†ç¢¼å³å¯ï¼Œå¾Œé¢ç¢°åˆ°ä¹Ÿæ˜¯ä¸€æ¨£)

å›åˆ° Windows Command Promptï¼Œä¸¦é—œé–‰WSLï¼š

```cmd
exit
wsl --shutdown
```

å†æ¬¡é–‹å•Ÿ Ubuntu (åœ¨ Command Prompt ä¸‹ï¼ŒåŸ·è¡Œ `wsl` å³å¯)ï¼Œä¸¦ç¢ºèª systemd æœ‰å•Ÿå‹•ï¼š


```bash
systemctl --version
```

çœ‹åˆ°ç‰ˆæœ¬è³‡è¨Šå°± OKã€‚


### 1.4 è‹¥æ˜¯ Data Center å¡ï¼Œè¦åˆ‡æ›æˆ MCDM æ¨¡å¼ (å¯é¸)

âš ï¸ é‡è¦ï¼šè‹¥æ‚¨çš„é¡¯ç¤ºå¡æ˜¯ GeForceã€Quadro ç­‰é€™é¡ã€Œæœ‰é¢¨æ‰‡è¨­è¨ˆã€çš„VGA Controller (å‡ºå» æ™‚è¨­å®šç‚ºé¡¯ç¤ºåŸ åŠŸèƒ½æ‰“é–‹)ï¼Œè«‹ç›´æ¥ç•¥éæœ¬æ­¥é©Ÿã€‚

âš ï¸ è‹¥æ‚¨çš„é¡¯ç¤ºå¡æ˜¯ L2ã€L4ã€L40ã€RTX 6000 Blackwell Server Edition ç­‰é€™é¡ Data Center å°ˆç”¨çš„ç‰ˆæœ¬ (é€™é¡ã€ŒåŠ é€Ÿå¡ã€çš„ç‰¹å¾µéƒ½æ˜¯ç„¡é¢¨æ‰‡è¨­è¨ˆï¼Œå‡ºå» æ™‚è¨­å®šç‚º 3D Controller æ¨¡å¼ï¼Œé¡¯ç¤ºå¡åŸ åŠŸèƒ½é—œé–‰ï¼Œé©åˆä¼ºæœå™¨ç’°å¢ƒ)ã€‚åœ¨ Windows ç’°å¢ƒä¸‹ï¼ŒNVIDIA Driver è‡ªå®šæœƒä»¥ TCC (Tesla Compute Cluster) æ¨¡å¼ä¾†é‹ä½œï¼Œæ­¤æ¨¡å¼æœƒè®“ Ubuntu WSL ç’°å¢ƒç„¡æ³•å­˜å–åˆ°é€™å¼µåŠ é€Ÿå¡ï¼Œå› æ­¤å¿…é ˆåˆ‡æ›æˆ MCDM (Microsoft Compute Driver Model) æ¨¡å¼ï¼Œæ‰èƒ½æ­£å¸¸é‹ä½œã€‚

è«‹ä»¥ **ç³»çµ±ç®¡ç†å“¡** åŸ·è¡Œ Command Prompt ï¼Œç„¶å¾Œä¸‹:

```cmd
# å°‡ L4 åˆ‡æ›åˆ° MCDM æ¨¡å¼ (-g 0 ç‚º GPU ç·¨è™Ÿï¼Œ-dm 2 ä»£è¡¨ MCDM)
nvidia-smi -g 0 -dm 2
```

åˆ‡æ›å®Œæˆå¾Œï¼Œéœ€è¦é‡æ–°é–‹æ©Ÿã€‚

é©—è­‰æ˜¯å¦åˆ‡æ›æˆåŠŸï¼Œåªéœ€è¦åœ¨ Command Prompt ä¸‹ï¼ŒåŸ·è¡Œ `nvidia-smi`æª¢è¦– Driver-Model é‚£é‚Šé¡¯ç¤ºç‚º MCDM ï¼Œå°±ç­‰æ–¼åˆ‡æ›å®Œæˆã€‚


> è‹¥æ—¥å¾Œä¸éœ€è¦ç”¨åˆ°æ™‚ï¼Œå†ä¸‹ä»¥ä¸‹å‘½ä»¤åˆ‡å› TCC æ¨¡å¼:

```cmd
# å°‡ GPU 0 åˆ‡æ›å› TCC æ¨¡å¼ (-dm 1 ä»£è¡¨ TCC)
nvidia-smi -g 0 -dm 1
```

### 1.5 å®‰è£ WSL Ubuntu å°ˆç”¨çš„ CUDA Driverï¼ˆå¿…è¦ï¼‰

> âš ï¸ å¿…åšæ­¥é©Ÿï¼šUbuntu WSL ä¸‹ä¹Ÿæ˜¯éœ€è¦å®‰è£ NVIDIA Driver ï¼Œæ‰èƒ½å­˜å–åˆ° Host Windows ç«¯çš„é¡¯ç¤ºå¡è³‡æºã€‚

å…ˆåˆ° [NVIDIA CUDA Toolkit](https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=WSL-Ubuntu&target_version=2.0&target_type=deb_local) ç¶²ç«™ï¼Œå–å¾—æœ€æ–° WSL-Ubuntu çš„ CUDA å®‰è£æŒ‡å¼•ï¼Œå…¶ä¸­ Installer Type å»ºè­°é¸ deb (local) ä¾†å®‰è£ã€‚

ä»¥ CUDA Toolkit 13.1 Update 1 ç‚ºä¾‹ï¼Œéœ€è¦åœ¨ WSL ä¸‹ï¼ŒåŸ·è¡Œä»¥ä¸‹çš„æŒ‡ä»¤ä¾†ä¾æ¬¡å®‰è£:
```bash
cd ~
wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin
sudo mv cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600
wget https://developer.download.nvidia.com/compute/cuda/13.1.1/local_installers/cuda-repo-wsl-ubuntu-13-1-local_13.1.1-1_amd64.deb
sudo dpkg -i cuda-repo-wsl-ubuntu-13-1-local_13.1.1-1_amd64.deb
sudo cp /var/cuda-repo-wsl-ubuntu-13-1-local/cuda-*-keyring.gpg /usr/share/keyrings/
sudo apt-get update
sudo apt-get -y install cuda-toolkit-13-1
```

è£å®Œ CUDA Toolkit ä¹‹å¾Œï¼Œè¨˜å¾—è¨­å®šå…©å€‹ç’°å¢ƒè®Šæ•¸ï¼Œå…ˆä½¿ç”¨ nano ä¿®æ”¹ `~/.bashrc` æª”æ¡ˆï¼š

```bash
nano ~/.bashrc
```

åœ¨æœ€å¾Œä¸€è¡Œé‚£é‚ŠåŠ å…¥ä»¥ä¸‹å…§å®¹:

```
export PATH="$PATH:/usr/local/cuda/bin:$HOME/.npm-global:$HOME/node_modules/.bin"
export LD_LIBRARY_PATH="$LD_LIBRARY_PATH:/usr/local/cuda/lib64"
```

> è¨»: ä¸Šé¢è·¯å¾‘æœ‰é å…ˆå¹« node, npm æ‰“å¥½ï¼Œä»¥ä¾¿ç¨å¾Œå®‰è£ OpenClaw æ™‚ç”¨åˆ°

æ”¹å¥½ä¹‹å¾Œï¼ŒæŒ‰ `Ctrl+X`ã€`Y`ã€`Enter`éˆ•ï¼ŒæŠŠæª”æ¡ˆå­˜å›ã€‚

æ¥è‘— è¼¸å…¥ `exit` é›¢é–‹ WSL ï¼Œå†è·‘ `wsl` å•Ÿå‹• Ubuntu ï¼Œè¼¸å…¥ä»¥ä¸‹æŒ‡ä»¤ä»¥é©—è­‰ CUDA å’Œ Driver æ˜¯å¦æ­£å¸¸é‹ä½œ:

```bash
# æª¢æŸ¥ nvcc æ˜¯å¦åŸ·è¡Œ
nvcc -V
# æª¢æŸ¥ nvidia-smi æ˜¯å¦æ­£å¸¸é‹ä½œ
nvidia-smi
```

> è¨»ï¼šé€²è¡Œåˆ°é€™é‚Šç´¯äº†å—? æ„Ÿè¦º WSL ç‰ˆæœ¬æ²’é‚£éº¼ç°¡å–®å§? ğŸ˜‚ğŸ˜‚ğŸ˜‚

---

## 2. å®‰è£ Ollama (è‹¥æ‚¨ä½¿ç”¨ Linux æˆ– macOSï¼Œè«‹ç›´æ¥é€™è£¡é–‹å§‹)

### 2.1 å®‰è£ Ollama (è£é€² Ubuntu WSL / Linux è£¡é¢ï¼Œåˆ¥è£åœ¨ Windows ä¸‹)

ç¢ºèªæœ‰å®‰è£å¥½ GPU é©…å‹•ç¨‹å¼å’Œ Compute å‡½å¼åº«ä¹‹å¾Œï¼Œè«‹é–‹å•Ÿ Linux Terminal ï¼Œè¼¸å…¥ï¼š

```bash
cd ~
sudo apt install zstd
curl -fsSL https://ollama.com/install.sh | sh
```


### 2.2 å•Ÿå‹• Ollama æœå‹™

Ollama å®‰è£å®Œæˆå¾Œï¼Œæœƒè‡ªå·±å•Ÿç”¨æœå‹™ã€‚è‹¥æ²’æœ‰çš„è©±ï¼Œå¯ä»¥åŸ·è¡Œä»¥ä¸‹æŒ‡ä»¤ä¾†å•Ÿç”¨ã€‚

```bash
sudo systemctl enable ollama
sudo systemctl start ollama
```


### 2.3 é©—è­‰å®‰è£

```bash
ollama -v
```


### 2.4 æ‹‰æ¨¡å‹

åƒè€ƒä¸» README è£¡é¢çš„èªªæ˜ï¼Œä¾ç…§ä½ é¡¯ç¤ºå¡çš„ VRAM å®¹é‡ï¼Œé¸ä¸€å€‹ï¼š

**é¸é … Aï¼šGLM 4.7 Flashï¼ˆæ¨è–¦ï¼Œ20GB+ VRAMï¼‰**

```bash
ollama pull glm-4.7-flash
```

**é¸é … Bï¼šMinistral 3:8bï¼ˆè¼•é‡ï¼Œ8GB+ VRAMï¼‰**

```bash
ollama pull ministral-3:8b
```

---

## 3. å®‰è£ OpenClaw

### 3.1 å…ˆå®‰è£ nvm èˆ‡ node.js v24 (æ–°ç‰ˆ OpenClaw å»ºè­°)

```bash
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash
source ~/.bashrc
nvm install 24
nvm use 24
```

### 3.2 å®‰è£ OpenClaw CLI

```bash
curl -fsSL https://openclaw.ai/install.sh | bash
```

è‹¥ä½¿ç”¨å®˜æ–¹å®‰è£è…³æœ¬å¤±æ•—ï¼Œè«‹æ”¹ä½¿ç”¨ npm install æ–¹å¼ç›´æ¥å®‰è£

```bash
npm install openclaw@latest
```

### 3.3 åˆå§‹åŒ– Onboard

OpenClaw å®‰è£å®Œæˆå¾Œï¼Œæœƒè‡ªå‹•é€²å…¥ Onboard æ¨¡å¼ã€‚è‹¥æ²’æœ‰çš„è©±ï¼Œå¯ä»¥åŸ·è¡Œä»¥ä¸‹æŒ‡ä»¤:

```bash
openclaw onboard
```

#### 1. å®‰å…¨ç¢ºèª

```
I understand this is powerful and inherently risky. Continue?
> Yes
```

#### 2. Onboarding æ¨¡å¼

```
Onboarding mode
> QuickStart (Configure details later via openclaw configure.)
```

#### 3. è¨­å®šæ¨¡å‹/èªè­‰æä¾›è€…

å…ˆé¸æ“‡è·³éï¼Œå¾Œé¢å†è¨­å®šï¼š

```
Model/auth provider
> Skip for now

Filter models by provider
> All providers

Default model
> Enter model manually 

#è¼¸å…¥ä¸Šè¿°çš„æ¨¡å‹åç¨±ï¼Œä¾‹å¦‚: ollama/glm-4.7-flash
```

#### 4. é »é“é…ç½®ï¼ˆå¯é¸ï¼‰

é€™è£¡å¯ä»¥å…ˆé¸æ“‡ **Skip for now**ï¼Œæˆ–ç›´æ¥é…ç½® Telegramï¼Œé€™è£¡ä»¥æœ‰é…ç½®ç‚ºä¾‹ï¼š

```
Select channel (QuickStart)
> â— Telegram (Bot API) (not configured)

Enter Telegram bot token
>>> 1234567890:ABCDEFGHIJKLMNOPQRSTUVWXYZ123456789
```

> ğŸ’¡ å¦‚ä½•å–å¾— Tokenï¼Ÿè«‹åƒè€ƒ [Telegram Bot è¨­å®š](#telegram-bot-è¨­å®š)

#### 5. æŠ€èƒ½å•†åº—

å…ˆé¸ Yesï¼Œç„¶å¾Œ Skip for now ï¼Œå¾Œé¢ API Key çš„éƒ¨ä»½éƒ½å…ˆé¸ No ã€‚

```
Configure skills now? (recommended)
> Yes

Install missing skill dependencies
> â—¼ Skip for now (Continue without installing dependencies)

Set GOOGLE_PLACES_API_KEY for goplaces?
> No

Set GOOGLE_PLACES_API_KEY for local-places?
> No

Set GEMINI_API_KEY for nano-banana-pro?
> No

Set NOTION_API_KEY for notion?
> No

Set OPENAI_API_KEY for openai-image-gen?
> No

Set OPENAI_API_KEY for openai-whisper-api?
> No

Set ELEVENLABS_API_KEY for sag?
> No
```


#### 6. å•Ÿç”¨ Hooks (è‹¥å‡ºç¾çš„è©±)

```
Enable hooks?
> [+] ğŸš€ boot-md (Run BOOT.md on gateway startup)
> [+] ğŸ“ bootstrap-extra-files (Injects additional workspace bootstrap files)
> [+] ğŸ“ command-logger (Log all command events to a centralized audit file)
> [+] ğŸ’¾ session-memory (Save session context to memory when /new command is issued)
```

æŒ‰**ç©ºç™½éµ**é¸æ“‡å…¨éƒ¨ä¸‰é …ï¼Œå†æŒ‰ **Enter**ã€‚


#### 7. è¨˜éŒ„ Web UI è³‡è¨Š

å®‰è£å®Œæˆå¾Œæœƒé¡¯ç¤ºï¼š

```
Control UI:
  Web UI: http://127.0.0.1:18789/
  Web UI (with token): http://127.0.0.1:18789/?token=xxxxxxxxxx
  Gateway WS: ws://127.0.0.1:18789
  Gateway: reachable
```

> ğŸ”‘ **é‡è¦**: è¨˜ä½å¸¶ token çš„ URLï¼

#### 8. å®‰è£ Shell / å­µåŒ– Bot (è‹¥å‡ºç¾çš„è©±)

```
Enable bash shell completion for openclaw?
> Yes
```

å¦‚ä½•å­µåŒ–æ‚¨çš„æ©Ÿå™¨äººï¼Œå»ºè­° TUI ã€‚

```
How do you want to hatch your bot?
> â— Hatch in TUI (recommended)
```

### 9. è¨­å®š OpenClaw ä½¿ç”¨ Ollama ###

æ­¤æ™‚ï¼ŒOpenClaw æœƒå­µåŒ–æ©Ÿå™¨äººï¼Œæ­¤æ™‚å…ˆæŒ‰ Ctrl+C éˆ•æ•¸æ¬¡åœæ­¢ï¼Œå…ˆé€€å‡º WSL (è¼¸å…¥ `exit`)ï¼Œç„¶å¾Œå†é€²å…¥ WSL (è¼¸å…¥ `wsl`)ï¼Œæ­¤æ™‚ OpenClaw å°±å¯ä»¥ä½¿ç”¨äº†ã€‚

```
# å…ˆåœæ­¢ OpenClaw Gateway æœå‹™
systemctl --user stop openclaw-gateway.service
```

#### è®“ Ollama å¥—ç”¨æœ¬åœ°ç«¯æ¨¡å‹ï¼Œå»åŸ·è¡Œ OpenClaw

**Ollama v0.15.3+ æ–°åŠŸèƒ½**: å¯é…ç½® OpenClaw çš„ Ollama è¨­å®šï¼Œè®“å…¶å¥—ç”¨æœ¬åœ°æ¨¡å‹ã€‚

```
ollama launch openclaw
```

é¸æ“‡æ‚¨çš„ Model:

```
Select models for OpenClaw: Type to filter...

  Recommended
  â–¸ [x] glm-4.7-flash (default)
      Reasoning and code generation locally
    [ ] qwen3:8b
      Efficient all-purpose assistant, ~11GB, (not downloaded)
    [ ] minimax-m2.5:cloud
      Fast, efficient coding and real-world productivity, (not downloaded)
    [ ] glm-5:cloud
      Reasoning and code generation, (not downloaded)
    [ ] kimi-k2.5:cloud
      Multimodal reasoning with subagents, (not downloaded)

  1 selected - press enter to continue

â†‘/â†“ navigate â€¢ space toggle â€¢ enter confirm â€¢ esc cancel
```

è¨­å®šå¥½æ¨¡å‹ä¹‹å¾Œï¼Œ OpenClaw å°±èƒ½æ­£å¼ä½¿ç”¨äº†ã€‚æ­¤æ™‚å°±å¯ä»¥é€é Telegram ç­‰ IM ä¾†èˆ‡OpenClaw äº¤è«‡äº†ã€‚

âœ… **è‹¥ AI æ­£å¸¸å›è¦†ï¼ŒåŸºæœ¬è¨­å®šå®Œæˆï¼**

> ğŸ“ **æ—¥å¾Œæ›´æ›æ¨¡å‹**: è«‹åƒè€ƒä¸‹æ–¹ **å¯¦ç”¨æŠ€å·§**ã€‚


> ğŸ’¡ **å»ºè­°**: åŸ·è¡Œå®Œå¾Œï¼Œå…ˆé‡æ–°å•Ÿå‹•é›»è…¦ï¼Œç¢ºä¿ Gateway Service æ­£å¸¸é–‹æ©Ÿè‡ªå‹•å•Ÿå‹•ã€‚

---

## 4. é€²éšé…ç½®

### 4.1 å®‰è£ Skills

ç¾åœ¨ä½ å·²åœ¨ Linuxï¼ˆWSL2ï¼‰å…§ï¼Œæ‰€ä»¥å¤§éƒ¨åˆ† skills éƒ½èƒ½åˆ¤å®šç‚ºå¯ç”¨ã€‚

å¸¸è¦‹æµç¨‹ï¼š

```bash
# æ‰¾ skills
clawhub search

# å®‰è£
clawhub install <skill-name>

# æª¢æŸ¥
openclaw doctor
```

å¦‚æœæœ‰éœ€è¦ Homebrew ç‰ˆæœ¬çš„ skillsï¼Œä½ å¯ä»¥é¸æ“‡ï¼š

```bash
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# åŠ åˆ° PATH
echo 'eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)"' >> ~/.bashrc
eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)"

brew --version
```

### 4.2 Memory åŠŸèƒ½

åœ¨ WSL2 ä¸‹ï¼Œmemory å­ç³»çµ±æ‡‰è©²æ­£å¸¸ï¼š

```bash
openclaw memory status
```

æœ‰å•é¡Œå¯ä»¥é‡å»ºï¼š

```bash
openclaw memory rebuild
```

---

## 4ï¸âƒ£ é€²éšé…ç½®

### Telegram Bot è¨­å®š

#### å»ºç«‹ Telegram æ©Ÿå™¨äºº

1. åœ¨ Telegram æœå°‹ä¸¦åŠ å…¥ **@BotFather**

2. ç™¼é€æŒ‡ä»¤ `/newbot`ï¼Œä¾æç¤ºè¨­å®šæ©Ÿå™¨äººåç¨±
   - ä¾‹å¦‚ï¼š`openclaw-bot`ï¼ˆè‹¥åç¨±è¢«ä½”ç”¨è«‹æ›´æ›ï¼‰

3. **BotFather** æœƒå›è¦†ï¼š

```
Done. Congratulations on your new bot...

Use this token to access the HTTP API:
1234567890:ABCDEFGHIJKLMNOPQRSTUVWXYZ123456789
```

> ğŸ”‘ **è¨˜ä½é€™å€‹ Token**ï¼Œç¨å¾Œé…ç½®æ™‚éœ€è¦ï¼

### é…å° Telegram é »é“

1. é€²å…¥æ‰‹æ©Ÿ Telegram ä¸­çš„ bot é »é“ï¼ŒæŸ¥çœ‹æ˜¯å¦æœ‰ä¸‹åˆ—è¨Šæ¯ ï¼ˆè‹¥æ²’æœ‰ï¼Œè«‹ç™¼é€ä»»æ„è¨Šæ¯)

```
OpenClaw: access not configured.

Your Telegram user id: 1234567890
Pairing code: abcdefgh
```

2. åœ¨é›»è…¦ä¸Šé€²å…¥ WSL ï¼ŒåŸ·è¡Œé…å°æŒ‡ä»¤ï¼š

```cmd
openclaw pairing approve telegram abcdefgh
```

ï¼ˆå°‡ `abcdefgh` æ›¿æ›æˆä½ çš„é…å°ç¢¼ï¼‰

3. å†æ¬¡ç™¼é€è¨Šæ¯æ¸¬è©¦

âœ… **Bot æ‡‰è©²å¯ä»¥æ­£å¸¸å›è¦†äº†ï¼** ğŸ‰

### å…¶ä»–é€²éšè¨­å®š (å¯é¸)

ä»¥ä¸€èˆ¬ä½¿ç”¨è€…èº«ä»½é–‹å•Ÿ WSLï¼š

```bash
openclaw config
```

#### Gateway ä½ç½®

```
Where will the Gateway run?
> â— Local (this machine)
```

#### Web å·¥å…·é…ç½®

```
Select sections to configure
> â— Web tools (Configure Brave search + fetch)

Enable web_search (Brave Search)?
> â—‹ Yes / â— No
```

> ğŸ’¡ éœ€è¦ Brave API Keyï¼ˆå¯å¦å¤–ç”³è«‹ï¼‰ï¼Œæš«æ™‚é¸ No

```
Enable web_fetch (keyless HTTP fetch)?
> â— Yes / â—‹ No
```

---

## ğŸ—‘ï¸ å®Œæ•´ç§»é™¤æŒ‡å—

è‹¥éœ€è¦å®Œå…¨ç§»é™¤ OpenClaw / Moltbot / Clawdbotï¼š

### é€²å…¥ WSL

```bash
# å®Œæ•´ç§»é™¤ï¼ˆåŒ…å«æ‰€æœ‰è³‡æ–™ï¼‰
openclaw uninstall --all --yes --non-interactive
# æˆ–
moltbot uninstall --all --yes --non-interactive
# æˆ–
clawdbot uninstall --all --yes --non-interactive

# ç§»é™¤ npm å¥—ä»¶
npm uninstall -g openclaw
# æˆ–
npm uninstall -g moltbot
# æˆ–
npm uninstall -g clawdbot
```

---

## ğŸ“„ é…ç½®æª”æ¡ˆåƒè€ƒ

### æª”æ¡ˆè·¯å¾‘

```
~/.openclaw/openclaw.json
```

### é…ç½®ç¯„ä¾‹

```json
{
  "models": {
    "providers": {
      "ollama": {
        "baseUrl": "http://127.0.0.1:11434/v1",
        "apiKey": "ollama-local",
        "api": "openai-completions",
        "models": [
          {
            "id": "ollama/glm-4.7-flash",
            "name": "GLM 4.7 Flash",
            "reasoning": true,
            "input": ["text"],
            "cost": {
              "input": 0,
              "output": 0,
              "cacheRead": 0,
              "cacheWrite": 0
            },
            "contextWindow": 32768,
            "maxTokens": 16384
          }
        ]
      }
    }
  },
  "agents": {
    "defaults": {
      "model": {
        "primary": "ollama/glm-4.7-flash",
        "fallbacks": ["ollama/gemini-3-flash-preview:cloud"]
      },
      "workspace": "/home/user/.openclaw/workspace",
      "compaction": {
        "mode": "safeguard"
      },
      "maxConcurrent": 4,
      "subagents": {
        "maxConcurrent": 8
      }
    }
  },
  "gateway": {
    "mode": "local",
    "auth": {
      "mode": "token",
      "token": "YOUR_TOKEN_HERE"
    },
    "port": 18789,
    "bind": "loopback"
  },
  "channels": {
    "telegram": {
      "enabled": true,
      "botToken": "YOUR_BOT_TOKEN_HERE"
    }
  },
  "hooks": {
    "internal": {
      "enabled": true,
      "entries": {
        "boot-md": { "enabled": true },
        "command-logger": { "enabled": true },
        "session-memory": { "enabled": true }
      }
    }
  }
}
```

---

## ğŸ¯ å¿«é€Ÿåƒè€ƒ

| æŒ‡ä»¤ | ç”¨é€” |
|------|------|
| `ollama --version` | æª¢æŸ¥ Ollama ç‰ˆæœ¬ |
| `ollama pull <model>` | æ‹‰å–æ¨¡å‹ |
| `ollama launch openclaw` | é…ç½® OpenClaw ä½¿ç”¨ Ollama |
| `openclaw config` | é€²å…¥é…ç½®ä»‹é¢ |
| `openclaw models list` | æª¢è¦–ç›®å‰é…ç½®çš„æ¨¡å‹åˆ—è¡¨ |
| `openclaw gateway install` | å®‰è£ Gateway æœå‹™ |
| `openclaw gateway start` | å•Ÿå‹• Gateway æœå‹™ |
| `openclaw pairing approve telegram <code>` | é…å° Telegram é »é“ |
| `openclaw security audit --deep` | å®‰å…¨æ€§æ·±åº¦æª¢æŸ¥ |
| `openclaw uninstall --all` | å®Œæ•´ç§»é™¤ |

---

## ğŸ’¡ å¯¦ç”¨æŠ€å·§

### 1. é˜²æ­¢ Ollama è‡ªå‹•å¸è¼‰æ¨¡å‹

Ollama è‡ªå®š 5 åˆ†é˜ç„¡æ´»å‹•å¾Œè‡ªå‹•å¸è¼‰æ¨¡å‹ï¼Œç‚ºæå‡ä¸‹æ¬¡å°è©±çš„é€Ÿåº¦ï¼Œå»ºè­°è¨­å®šæˆæ°¸ä¸å¸è¼‰ã€‚

#### Linux (Systemd):

åŸ·è¡Œ

```
sudo nano /etc/systemd/system/ollama.service
```

åœ¨ [Service] å€æ®µåŠ å…¥ï¼š

```
Environment="OLLAMA_KEEP_ALIVE=-1"
```

é‡è¼‰ä¸¦é‡å•Ÿï¼š

```
sudo systemctl daemon-reload
sudo systemctl restart ollama
```

#### macOS:

çµ‚ç«¯æ©ŸåŸ·è¡Œï¼š
```
launchctl setenv OLLAMA_KEEP_ALIVE "-1"
```

ç„¶å¾Œé‡å•Ÿ Ollama æ‡‰ç”¨ç¨‹å¼ã€‚ 


### 2. èª¿æ•´ Ollama çš„ä¸Šä¸‹æ–‡é•·åº¦

Ollama çš„ Context Length è‡ªå®šå€¼æ˜¯ 4096 ï¼Œå° OpenClaw ä¾†èªªå¯¦åœ¨å¤ªå°‘äº†ã€‚å»ºè­°èª¿é«˜è‡³ 16384ï½32768 ä»¥ä¸Š (æ³¨æ„ï¼šå¢åŠ  Context Sizeä¹Ÿæœƒå¢åŠ GPU VRAMçš„è€—ç”¨é‡)ï¼Œå¯ä»¥é€éä¸‹åˆ—æŒ‡ä»¤ä¾†ä¿®æ”¹æ¨¡å‹ã€‚

```
# åŸ·è¡Œ Ollama ä¸¦è¼‰å…¥æ¨¡å‹
ollama run glm-4.7-flash:latest

# è¨­å®šä¸Šä¸‹æ–‡é•·åº¦ (å»ºè­° 16384 æˆ– 32768)
>>> /set parameter num_ctx 32768

# å„²å­˜æ–°æ¨¡å‹ï¼Œå¯è¼¸å…¥åŸå…ˆæ¨¡å‹åç¨± (glm-4.7-flash:latest) ç›´æ¥æ›´æ–°
>>> /save <model name>

# é›¢é–‹ Ollama
>>> /bye
```

é©—è­‰æ˜¯å¦æ›´æ–° Context Size:

```
# åŸ·è¡Œ Ollama ä¸¦è¼‰å…¥æ¨¡å‹
ollama run glm-4.7-flash:latest

# é›¢é–‹ Ollama
>>> /bye

# æª¢è¦– Ollama è¼‰å…¥ç‹€æ³
ollama ps

NAME                    ID              SIZE     PROCESSOR    CONTEXT    UNTIL
glm-4.7-flash:latest    baa9f0d690c1    22 GB    100% GPU     32768      Forever
```

æ³¨æ„çœ‹ CONTEXT å’Œ UNTIL å°±å¯ä»¥äº†ã€‚


### 3. æ›´æ–° Ollama æ¨¡å‹é…ç½®

è‹¥éœ€è¦æ›´æ› Ollama æ¨¡å‹ï¼š

1. åˆªé™¤ Ollama é…ç½®æª”ï¼š

   ```bash
   rm ~/.ollama/config/config.json
   ```

2. é‡æ–°åŸ·è¡Œé…ç½®ï¼š

   ```cmd
   ollama launch openclaw
   ```

---

## ğŸ“š ç›¸é—œé€£çµ

- [ğŸ‘ Windows åŸºæœ¬å®‰è£æŒ‡å—](../README.md)
- [ğŸ”„ å¾ Windows é·ç§»åˆ° WSL2](migration-guide.md)
- [ğŸ¤” ç‚ºä»€éº¼éœ€è¦ WSL2?](why-wsl2.md)

- [ğŸ¦™ Ollama å®˜ç¶²](https://ollama.com/)
- [ğŸ¦ OpenClaw å®˜ç¶²](https://openclaw.ai/)
- [ğŸ¦ OpenClaw æ–‡ä»¶ - Ollama è¨­å®š](https://docs.openclaw.ai/providers/ollama)
- [ğŸ¤– Telegram BotFather](https://t.me/BotFather)

---

## ğŸ’¬ ç¤¾ç¾¤æ”¯æ´

é‡åˆ°å•é¡Œï¼Ÿæ­¡è¿åœ¨ [GitHub Issues](https://github.com/anomixer/openclaw-setup/issues) æå‡ºï¼

---

## ğŸ“ æ›´æ–°æ—¥èªŒ

### 2026-02-21
- ğŸš€ æ›´æ–° Node å®‰è£æ–¹æ³•
- ğŸ†• æ”¯æ´æœ€æ–° Ollama 0.16.1+ èˆ‡ OpenClaw 2026.2.21+ ç‰ˆæœ¬

### 2026-02-13
- ğŸš€ æ›´æ–° WSL å®‰è£æ–¹æ³•
- ğŸ†• æ”¯æ´æœ€æ–° Ollama 0.15.6+ èˆ‡ OpenClaw 2026.2.12+ ç‰ˆæœ¬

### 2026-02-05
- ğŸš€ æ”¹ç”¨ `cmd` å¿«é€Ÿå®‰è£æŒ‡ä»¤ï¼Œè‡ªå‹•åŒ–å®‰è£ Node.js èˆ‡ npm
- ğŸ†• æ”¯æ´æœ€æ–° OpenClaw 2026.2.5+ ç‰ˆæœ¬
- ğŸ“‹ é‡ç·¨ç›®éŒ„ä¸¦æ›´æ–°ç¿»è­¯è‡³ `README-EN.md`

### 2026-02-02
- ğŸ”„ æ›´æ–°è‡³ Ollama v0.15.4+ ç‰ˆæœ¬
- âœ¨ æ–°å¢ `ollama launch openclaw` é é…ç½®åŠŸèƒ½
- ğŸ“– é‡æ§‹æ–‡ä»¶çµæ§‹ï¼Œæå‡å¯è®€æ€§
- âš ï¸ å¼·èª¿å¿…é ˆä½¿ç”¨ä¸€èˆ¬ä½¿ç”¨è€…èº«ä»½å®‰è£

### 2026-01-30
- ğŸ¦ Repo é‡æ–°å‘½åç‚º openclaw-setup
- ğŸŒ æ–°å¢è‹±æ–‡ç‰ˆ README
- ğŸ’¬ æ–°å¢ murmur.md åæ§½æª”æ¡ˆ


---

## ç›¸é—œé€£çµ

- [ä¸» README](../README.md) - Windows åŸç”Ÿæ•™å­¸
- [ç‚ºä»€éº¼éœ€è¦ WSL2ï¼Ÿ](why-wsl2.md) - æŠ€è¡“èƒŒæ™¯
- [å¾ Windows é·ç§»åˆ° WSL2](migration-guide.md)
- [WSL2 å®˜æ–¹æ–‡ä»¶](https://learn.microsoft.com/windows/wsl/)

---

**æœ€å¾Œæ›´æ–°**: 2026-02-21
**åŸå‰µ by anomixer**  
**Clawdbot â†’ Moltbot â†’ OpenClaw**
